# ghostcode/types.py
from typing import *
from dataclasses import dataclass, field
from enum import Enum, StrEnum
from contextlib import contextmanager
from pydantic import BaseModel, Field, model_validator
import os
import atexit
from uuid import uuid4, UUID
import subprocess
import traceback
import json
import yaml
import logging
import ghostbox.definitions
from ghostbox.definitions import LLMBackend
import ghostcode
from ghostcode.ansi_colors import Color256, colored
from ghostbox import Ghostbox
from ghostcode.soundmanager import SoundManager
from ghostcode.utility import (
    language_from_extension,
    timestamp_now_iso8601,
    show_model,
    show_model_nt,
    show_model_nt,
    PydanticEnumDumper,
    make_mnemonic,
    quoted_if_nonempty,
)
from ghostcode import shell
import appdirs  # Added for platform-specific config directory
from . import git

# --- Logging Setup ---
# Configure a basic logger for the ghostcode project
logger = logging.getLogger("ghostcode.types")


### Protocols ###


class Showable(Protocol):
    """Interface for anything that can be turned into strings.
    By convention, the CLI method turns objects into strings in a way that is slightly less verbose and more friendly to humans, though it is not required that show and show_cli have different return values at all.
    """

    def show_cli(self, **kwargs: Any) -> str:
        pass

    def show(self, **kwargs: Any) -> str:
        pass


class HasTimestamp(Protocol):
    timestamp: str


class HasUniqueID(Protocol):
    unique_id: str
    tag: Optional[str]


class HasGitCommitHash(Protocol):
    git_commit_hash: Optional[str]
    applied_commit_hash: Optional[str]


class HasClearance(Protocol):
    """Interface for anything that requires clearance."""

    clearance_required: ClassVar["ClearanceRequirement"]


class CanBeChatMessage(Protocol):
    def to_chat_message(self) -> ghostbox.ChatMessage:
        pass


### --- Default Configurations ---
# Default configuration for the coder LLM (e.g., for planning, complex reasoning)
DEFAULT_CODER_LLM_CONFIG = {
    "temperature": 0.2,
    "max_length": -1,
    "top_p": 0.9,
    "log_time": True,
    "verbose": False,
    "chat_ai": "GhostCoder",
    "stdout": False,
    "stderr": False,
    "quiet": True,
    "stream": False,
    "history_retroactive_vars": True,
}

# Default configuration for the worker LLM (e.g., for generating code snippets, answering questions)
DEFAULT_WORKER_LLM_CONFIG = {
    "temperature": 0.7,
    "max_length": -1,
    "top_p": 0.95,
    "log_time": True,
    "verbose": False,
    "stdout": False,
    "stderr": False,
    "quiet": True,
    "stream": False,
    "chat_ai": "GhostWorker",
    "history_retroactive_vars": True,
}

# Default project metadata
DEFAULT_PROJECT_METADATA = {
    "name": "New Ghostcode Project",
    "description": "A new project initialized with ghostcode. Edit this description to provide an overview of your project.",
}


class LLMResponseProfile(BaseModel):
    """Constrain the types of responses generated by an LLM.
    These really are constraints, i.e. purely negative. If you set images to True with an LLM that has no image generation capability, you will not get an image.
    """

    text: bool = Field(default=True, description="The LLM may generate text responses.")

    images: bool = Field(
        default=True, description="The LLM may generate images. Currently unused."
    )

    tools: bool = Field(
        default=True, description="The LLM may call tools. Currently unused."
    )

    audio: bool = Field(
        default=True, description="The LLM may generate audio. Currently unused."
    )

    video: bool = Field(
        default=True, description="The LLM may generate video. Currently unused."
    )

    actions: bool = Field(
        default=True,
        description="The LLM may generate actions. This is a way to do tool calling through structured output.",
    )

    @staticmethod
    def allow_all() -> "LLMResponseProfile":
        return LLMResponseProfile()

    @staticmethod
    def forbid_all() -> "LLMResponseProfile":
        """Returns an LLMResponseProfile that disables all response types."""
        # it is good to be lazy
        default = LLMResponseProfile()
        d = default.model_dump()
        # we can just do this and if it breaks in the future pydantic will warn us
        for k, v in d.items():
            d[k] = False

        return LLMResponseProfile(**d)

    @staticmethod
    def text_only() -> "LLMResponseProfile":
        p = LLMResponseProfile.forbid_all()
        p.text = True
        return p


class ClearanceRequirement(Enum):
    """Defines the level of user clearance required for an action.
    Higher values indicate a higher requirement for user interaction/permission.
    """

    AUTOMATIC = 10  # Actions that are generally safe, non-destructive, or purely informational (e.g., logging, internal state changes). Can proceed without explicit user confirmation.
    INFORM = 20  # Actions that are safe but the user should be made aware of, typically non-critical file creations (e.g., temporary files, new log files) or minor, reversible changes. User is informed, but no explicit confirmation is strictly required unless configured otherwise.
    CONFIRM = 30  # Actions that modify existing user-created files, create new permanent files, or execute commands that might have noticeable side effects (e.g., `pip install`). Requires explicit 'yes/no' user confirmation by default.
    DANGEROUS = 40  # Actions with significant potential for data loss, system instability, or execution of arbitrary, untrusted code (e.g., `rm -rf`, running downloaded scripts). Requires strong user confirmation (e.g., typing the full word 'confirm').
    FORBIDDEN = 50  # Actions that the AI is explicitly forbidden from performing under any circumstances. Attempting these actions should result in an immediate halt and error.


class UserConfirmation(Enum):
    """Possible results of a user confirmation dialog."""

    YES = 1
    NO = 2
    ALL = 3
    CANCEL = 4
    SHOW_MORE = 5

    @staticmethod
    def is_confirmation(value: "UserConfirmation") -> bool:
        return value in [UserConfirmation.YES, UserConfirmation.ALL]


class AIAgent(StrEnum):
    WORKER = "worker"
    CODER = "coder"


class UserConfig(BaseModel):
    """Stores user specific data, like names, emails, and api keys.
    Usually stored in a .ghostcodeconfig file, in a platform specific location. The user settings are used across multiple ghostcode projects, and so aren't stored in the .ghostcode project folder.
    """

    name: str = ""
    email: str = ""
    api_key: str = Field(
        default="",
        description="The most general API key. This is used for a LLM backend service that requires an API key if none of the specific API keys are set.",
    )

    generic_model_coder: str = Field(
        default = "",
        description = "The model t ouse when using the 'generic' backend with the coder LLM. The generic backend targets an OpenAI compatible API and is used e.g. with local LLMs, where model selection is usually not available, so in many cases this parameter is unused."
    )

    generic_model_worker: str = Field(
        default = "",
        description = "The model t ouse when using the 'generic' backend with the worker LLM. The generic backend targets an OpenAI compatible API and is used e.g. with local LLMs, where model selection is usually not available, so in many cases this parameter is unused."
    )
    
    openai_api_key: str = Field(
        default="",
        description="Used with Chat-GPT and the OpenAI LLM backend. Get your API key at https://openai.com\nIf this key is set and you use openai as your backend, it will have precedence over the generic api_key.",
    )
    
    openai_model_coder: str = Field(
        default = "Chat-GPT-5.1",
        description = "Which model to use when querying the OpenAI backend with the coder LLM."
    )

    openai_model_worker: str = Field(
        default = "Chat-GPT-5.1",
        description = "Which model to use when querying the OpenAI backend with the worker LLM."
    )    
    
    google_api_key: str = Field(
        default="",
        description="Use with Gemini and the Google AI Studio API. Get your API key at https://aistudio.google.com\nIf this key is set and you use google as your backend, it will have precedence over the generic api_key.",
    )

    google_model_coder: str = Field(
        default = "models/gemini-2.5-flash",
        description = "Model to use when querying the google AI Studio backend with the coder LLM."
    )

    google_model_worker: str = Field(
        default = "models/gemini-2.0-flash",
        description = "Model to use when querying the google AI Studio backend with the worker LLM."
    )
    
    deepseek_api_key: str = Field(
        default = "",
        description = "API for deepseek https://deepseek.com"
    )

    deepseek_model_coder: str = Field(
        default = "deepseek-reasoner",
        description = "The deepseek model to user for the coder backend."
    )

    deepseek_model_worker: str = Field(
        default = "deepseek-reasoner",
        description = "The deepseek model to user for the worker backend."
    )
    
    newbie: bool = Field(
        default=True,
        description="If true, ghostcode will occasionally print out helpful tips and be generally more verbose and nanny-like. Finding this setting and successfully setting it to False is your trial-by-fire to transcend the newbie stage.",
    )

    sound_enabled: bool = Field(
        default = True,
        description = "If true, the CLI will play interface sounds, e.g. to signal when a response is ready."
    )

    sound_volume: float = Field(
        default = 1.0,
        description = "Factor to scale the interface sound volume. 1.0 means normal volume, 2.0 means twice as loud, 0.5 is half volume."
    )
    
    _GHOSTCODE_CONFIG_FILE: ClassVar[str] = ".ghostcodeconfig"

    def save(self, user_config_path: Optional[str] = None) -> None:
        """
        Saves the UserConfig instance to a YAML file.

        Args:
            user_config_path (Optional[str]): The full path to save the config file.
                                              If None, defaults to a platform-specific user config directory.
        """
        if user_config_path is None:
            config_dir = appdirs.user_config_dir("ghostcode")
            os.makedirs(config_dir, exist_ok=True)
            save_path = os.path.join(config_dir, self._GHOSTCODE_CONFIG_FILE)
        else:
            save_path = os.path.abspath(user_config_path)
            os.makedirs(os.path.dirname(save_path), exist_ok=True)

        logger.info(f"Saving user configuration to {save_path}")

        # Add helpful comments to the YAML output
        config_data = self.model_dump()
        yaml_content = f"""# Ghostcode User Configuration
# This file stores your personal settings, including API keys,
# which are used across all your ghostcode projects.
#
# API Keys:
#   - 'api_key': General API key for LLM services if no specific key is provided.
#   - 'openai_api_key': Specific API key for OpenAI (e.g., GPT-4o).
#     Get it from https://platform.openai.com/account/api-keys
#   - 'google_api_key': Specific API key for Google AI Studio (e.g., Gemini).
#     Get it from https://aistudio.google.com/app/apikey
#
# To fill in an API key, replace the empty string "" with your actual key.
# Example:
# openai_api_key: "sk-YOUR_OPENAI_API_KEY_HERE"
#
"""
        yaml_content += yaml.dump(config_data, indent=2, sort_keys=False)

        try:
            with open(save_path, "w") as f:
                f.write(yaml_content)
            logger.debug(f"User configuration saved successfully to {save_path}")
        except Exception as e:
            logger.error(
                f"Failed to save user configuration to {save_path}: {e}", exc_info=True
            )
            raise

    @staticmethod
    def load(user_config_path: Optional[str] = None) -> "UserConfig":
        """
        Loads a UserConfig instance from a YAML file.

        Args:
            user_config_path (Optional[str]): The full path to load the config file from.
                                              If None, defaults to a platform-specific user config directory.

        Returns:
            UserConfig: The loaded UserConfig instance.

        Raises:
            FileNotFoundError: If the config file does not exist at the specified or default path.
            yaml.YAMLError: If there's an error parsing the YAML content.
        """
        if user_config_path is None:
            config_dir = appdirs.user_config_dir("ghostcode")
            load_path = os.path.join(config_dir, UserConfig._GHOSTCODE_CONFIG_FILE)
        else:
            load_path = os.path.abspath(user_config_path)

        logger.info(f"Attempting to load user configuration from {load_path}")

        if not os.path.isfile(load_path):
            logger.debug(f"User configuration file not found at {load_path}")
            raise FileNotFoundError(f"User configuration file not found at {load_path}")

        try:
            with open(load_path, "r") as f:
                config_data = yaml.safe_load(f)
            if config_data is None:
                logger.warning(
                    f"User configuration file {load_path} is empty. Returning default UserConfig."
                )
                return UserConfig()

            user_config = UserConfig(**config_data)
            logger.debug(f"User configuration loaded successfully from {load_path}")
            return user_config
        except yaml.YAMLError as e:
            logger.error(f"Error decoding YAML from {load_path}: {e}", exc_info=True)
            raise
        except Exception as e:
            logger.error(
                f"An unexpected error occurred loading {load_path}: {e}", exc_info=True
            )
            raise

    def get_model(self, agent: AIAgent, backend: str | LLMBackend) -> str:
        """Returns the model configured for the given backend."""
        if isinstance(backend, str):
            final_backend: LLMBackend = LLMBackend[backend]
        else:
            final_backend = backend

        match final_backend:
            case LLMBackend.generic:
                if agent == AIAgent.CODER:
                    return self.generic_model_coder
                else:
                    return self.generic_model_worker
            case LLMBackend.google:
                if agent == AIAgent.CODER:
                    return self.google_model_coder
                else:
                    return self.google_model_worker
            case LLMBackend.deepseek:
                if agent == AIAgent.CODER:
                    return self.deepseek_model_coder
                else:
                    return self.deepseek_model_worker
            case _:
                logger.warning(f"Unsuported backend {final_backend} for model selection.")
                return ""
            
                    
class ProjectConfig(BaseModel):
    """Contains project wide configuration obptions.
    This is stored in project_root/config.yaml"""

    # Changed these to str for easier human readability/editing in YAML
    coder_backend: str = Field(
        default=ghostbox.definitions.LLMBackend.google.name,
        description="Backend for the Coder LLM (e.g., 'google', 'openai', 'generic').",
    )
    worker_backend: str = Field(
        default=ghostbox.definitions.LLMBackend.generic.name,
        description="Backend for the Worker LLM (e.g., 'llamacpp', 'generic').",
    )
    coder_endpoint: str = Field(
        default="https://generativelanguage.googleapis.com/v1beta",
        description="Endpoint for the Coder LLM.",
    )
    worker_endpoint: str = Field(
        default="http://localhost:8080", description="Endpoint for the Worker LLM."
    )

    coder_clearance_level: ClearanceRequirement = Field(
        default=ClearanceRequirement.INFORM,
        description="The clearance level that the coder LLM has by default. If the coder's clearance level meets or exceeds the clearance requirement for a given action, it is permitted to perform that action without user confirmation.",
    )

    worker_clearance_level: ClearanceRequirement = Field(
        default=ClearanceRequirement.INFORM,
        description="The clearance level that the worker LLM has by default. If the worker's clearance level meets or exceeds the clearance requirement for a given action, it is permitted to perform that action without user confirmation.",
    )

    shell_wait_time_minimum: Optional[float] = Field(
        default=1.0,
        description="Minimum time (in seconds) that the worker will wait for a shell command to execute, or None for no minimum.",
    )

    shell_wait_time_maximum: Optional[float] = Field(
        default=240.0,
        description="Maximum time (in seconds) that the worker will wait for a shell command to finish, or None for no limit.",
    )

    prompt_log_lines: int = Field(
        default=50,
        description="How many lines from the tail of the logs to show to an LLM backend when prompting.",
    )

    git_integration: bool = Field(
        default = True,
        description = "When False, ghostcode will ignore any git repositories that are coexisting with it. You will lose branch and commit information on interactions, and many other features."
    )

# --- Type Definitions ---
class ContextFileConfig(BaseModel):
    """Specify options, metadata, and special behaviours for certain files in the context."""

    rag: bool = Field(
        default=False,
        description="Whether to enable retrieval augmented generation for this file. Enabling RAG means the local LLM will retrieve only parts of the file if it deems it necessary with the given prompt. This is usually done for large text files or documentation, and helps to avoid huge token counts for the cloud LLM.",
    )

    locked: bool = Field(
        default = False,
        description = "If true, the file cannot be removed from the context before it is unlocked."
    )

    ignore: bool = Field(
        default = False,
        description = "Ignored context files will not have their contents be shown in any query."
    )

    summary: str = Field(
        default = "",
        description = "A summary of the file. Summaries are intended to be generated in the background while the user is idle in an interact session. This has not been implemented yet."
    )
    
class ContextFile(BaseModel):
    """Abstract representation of a filepath along with metadata. Context files are sent to the cloud LLM for prompts."""

    filepath: str = Field(
        description="The filepath to the context file. This is always relative to the directory that the .ghostcode directory is in (the root). This value is is used to show a filepath to both humans and the LLM."
    )

    abs_filepath: str = Field(
        description = "The absolute filepath to this file. This is used to actually get the file contents with e.g. show()."
    )

    config: ContextFileConfig = Field(
        default_factory = ContextFileConfig,
        description = "Additional information, metadata, options, and kspecial behaviour for this context file."
    )

    def get_abs_filepath(self) -> str:
        """Get the abs path. Helps narrow the type."""
        return self.abs_filepath
    
class ContextFiles(BaseModel):
    """Encapsulates the files (both code and otherwise) that are tracked by the project. Tracked files are sent to the cloud LLM with the prompt.
    To keep it simple and human-readable, filepaths are stored in .ghostcode/context_files, one per line, all relative to the directory that the .ghostcode directory is in. This file may change frequently.
    A separate .ghostcode/context_files_options stores metadata about the individual files.
    """

    data: List[ContextFile] = Field(default_factory=list)
    root: str = Field(
        description = "Filepath to the ghostcode project root."
    )
    
    def add(self, filepath: str) -> None:
        """Adds a filepath to the context."""
        self.data.append(
            ContextFile(
                filepath = filepath,
                abs_filepath = os.path.join(self.root, filepath)
            )
        )

    def has_filepath(self, filepath: str) -> bool:
        """Returns true if a given relative filepath is in the context."""
        for cf in self.data:
            if cf.filepath == filepath:
                return True
        return False
            
    def to_plaintext(self) -> str:
        """Serializes the list of context file paths to a plaintext string, one path per line."""
        return "\n".join([cf.filepath for cf in self.data])

    @staticmethod
    def from_plaintext(content: str, *, root: str) -> "ContextFiles":
        """Deserializes a plaintext string into a ContextFiles object.
        Each line is treated as a filepath."""
        filepaths = [line.strip() for line in content.splitlines() if line.strip()]
        context_files = [ContextFile(filepath=fp, abs_filepath=os.path.join(root, fp)) for fp in filepaths]
        return ContextFiles(data=context_files, root = root)

    def show(self, heading_level: int = 3, **kwargs: Any) -> str:
        """Renders file contents of the context files to a string, in a format that is suitable for an LLM."""
        w = ""
        for context_file in self.data:
            if context_file.config.ignore:
                continue
            try:
                with open(context_file.abs_filepath, "r") as f:
                    contents = f.read()
            except Exception as e:
                logger.error(
                    f"Could not read file {context_file.filepath}. Skipping. Reason: {e}"
                )
                continue

            w += (
                (heading_level * "#")
                + f""" {context_file.filepath}

```{language_from_extension(context_file.filepath)}
{contents}
```

"""
            )
        return w

    def show_cli(self, **kwargs: Any) -> str:
        """Shows the list of filepaths in a short, command line interface friendly manner."""
        return "(" + " ".join([item.filepath for item in self.data if not item.config.ignore]) + ")"

    def show_overview(self) -> str:
        """Returns a tab separated display of context files with their configuration options."""
        if not self.data:
            return "No context files tracked."

        # Calculate maximum filepath length for padding
        max_filepath_len = 0
        for cf in self.data:
            max_filepath_len = max(max_filepath_len, len(cf.filepath))

        # Build the header
        header = f"{'Filepath'.ljust(max_filepath_len)}	{'RAG'}	{'Locked'}	{'Ignored'}	{'Summary'}"
        separator = f"{'-' * max_filepath_len}	{'-'*3}	{'-'*6}	{'-'*7}	{'-'*7}"
        
        lines = [header, separator]

        # Build data rows
        for cf in self.data:
            filepath_padded = cf.filepath.ljust(max_filepath_len)
            rag_str = "Yes" if cf.config.rag else "No"
            locked_str = "Yes" if cf.config.locked else "No"
            ignore_str = "Yes" if cf.config.ignore else "No"
            summary_str = "Yes" if cf.config.summary else "No"

            lines.append(f"{filepath_padded}	{rag_str}	{locked_str}	{ignore_str}	{summary_str}")

        return "\n".join(lines)

        
    def set_config(self, filepath: str, config: ContextFileConfig) -> None:
        for cf in self.data:
            if cf.filepath == filepath:
                cf.config = config
            
    def add_or_alter(self, filepath: str, config: Optional[ContextFileConfig] = None) -> None:
        """Add a single filepath to the context during program execution.
        If the filepath already exists in context, it is replaced with the supplied file instead, possibly changing its attributes.
        """
        logger.debug(f"Trying to add {filepath} to context.")
        context_file = ContextFile(
            filepath=filepath,
            abs_filepath=os.path.join(self.root, filepath),
            config = config if config is not None else ContextFileConfig()
        )
        
        for i in range(0, len(self.data)):
            if self.data[i].filepath == filepath:
                logger.info(
                    f"Changing properties for context file {context_file.filepath}"
                )
                self.data[i] = context_file
                return

        # we did not find it
        logger.info(f"Adding {context_file.filepath} to context.")
        self.data.append(context_file)


                

    def remove(self, filepath: str) -> bool:
        """Remove a file from context during program execution.
        Returns true if the file was not found or locked."""
        for i in range(0, len(self.data)):
            if self.data[i].filepath == filepath:
                if self.data[i].config.locked:
                    logger.debug(f"Not removing {self.data[i].filepath} from context because it is locked.")
                    continue
                else:
                    logger.debug(f"Removing {filepath} from context.")
                    del self.data[i]
                return False

        logger.warning(
            f"Tried to remove {filepath} from context but no such file was found."
        )
        return True


class ProjectMetadata(BaseModel):
    name: str
    description: str

    def show(self, **kwargs: Any) -> str:
        """Returns a human-readable string representation of the project metadata."""
        return f"## Project Metadata\n{show_model(self)}\n"


### ResponseParts ###
# ResponseParts represent structured data that can be sent back by an LLM backend.
# These types are generally shared between coder and worker backends, though they both use their own subsets for during any particular request, and may use a subset of these that is appropriate to the particular request and context in which it happens.
# See also the various Action types further below.


class CodeResponsePart(BaseModel):
    """Represents a segment of programming code generated by the LLM backend.
    This can be either a full file replacement or a partial modification within an existing file.

    Crucially, if the 'type' is 'partial', the 'original_code' field MUST be accurately provided.
    This 'original_code' block is used by GhostWorker to precisely locate the section of code
    to be replaced within the specified 'filepath'. Without an exact or very close match
    for 'original_code', the partial replacement operation will fail.

    Note to ghostcoder: Use this response part when you want to generate code that should replace existing files in the context or create new files.
    """

    type: Literal["full", "partial"] = Field(
        default="full",
        description="Indicates whether this code response part is a full or partial code block.\nFull code blocks are intended to replace entire files, and do not require an 'original_code' block.\nA partial response replaces only a specific segment of a file, and therefore *must* include the 'original_code' block that is to be replaced, along with the 'new_code' replacement. The 'original_code' should include a few lines of context before and after the actual change to aid precise location.",
    )

    filepath: Optional[str] = Field(
        default=None,
        description="The path to the file where this code generation belongs or should be saved.\nThis path is always relative to the project root. If None, the code is a free-standing snippet not intended for file modification.",
    )

    language: str = Field(
        default="",
        description="The programming language of the code (e.g., 'python', 'javascript', 'markdown').",
    )

    original_code: Optional[str] = Field(
        default=None,
        description="The exact original code block that is to be replaced by 'new_code'.\nThis field is MANDATORY and must be accurate if 'type' is 'partial'. It should include the surrounding lines of code (e.g., 2-3 lines before and after the actual change) to provide sufficient context for GhostWorker to locate the block reliably within the file.",
    )

    new_code: str = Field(
        default="",
        description="The actual new or modified code as plaintext that will replace 'original_code' (for partial) or the entire file (for full).",
    )

    context_anchor: Optional[str] = Field(
        default=None,
        description="A descriptive string identifying the containing syntactic construct (e.g., 'class MyClass:', 'def my_function(arg):', '# Main loop').\nThis helps GhostWorker narrow down the search area for 'original_code' in complex or large files, especially when multiple identical code snippets might exist.",
    )
    notes: List[str] = Field(
        default_factory=lambda: [],
        description="Concise notes, phrased as pull request comments, explaining the rationale, design decisions, assumptions, or potential implications of the generated code. This helps with code review and understanding the intent. Can be left empty if the change is self-explanatory. Remember that commit messages use present imperative tense.",
    )

    title: str = Field(
        default="",
        description="A short, descriptive title for the code block or the change it represents.",
    )

    def show_cli(self, **kwargs: Any) -> str:
        """Return a (short) representation that is suitable for the CLI interface."""
        filepath_str = f"({self.filepath})" if self.filepath else ""
        notes_str = (
            "\n" + "\n".join([f" - {note}" for note in self.notes])
            if self.notes
            else ""
        )
        return f"## {self.title} {filepath_str}{notes_str}"

    def show(self, include_code: bool = True, **kwargs: Any) -> str:
        missing_msg = "<Code Redacted>"
        """Returns a comprehensive string representation of the part."""
        filepath_str = f" ({self.filepath})" if self.filepath else ""
        notes_str = (
            "\n" + "\n".join([f" - {note}" for note in self.notes])
            if self.notes
            else ""
        )
        anchor_str = f"\nacnhor: {self.context_anchor}\n" if self.context_anchor else ""
        if self.original_code is not None:
            original_code_str = f"""

### Original Code
            
```{self.language}
{self.original_code if include_code else missing_msg}
```
            
"""
        else:
            original_code_str = ""

        return f"""## {self.title}{filepath_str}{anchor_str}{notes_str}{original_code_str}
        {"### New Code" if original_code_str else ""}
        
```{self.language}
        {self.new_code if include_code else missing_msg}
```
"""


class TextResponsePart(BaseModel):
    """A text chunk that is part of a response by the LLM backend.
    Use this for general discussions, explanations, open questions, brainstorming, warnings, or any information not directly tied to a code change.
    Examples include: overall architectural thoughts, design decisions, potential issues, alternative approaches, or responses to user questions that don't require code.
    Note to ghostworker: Use this when you want to tell the user something. This is the only response part that will be directly displayed to the user.
    """

    text: str = Field(default="", description="The text payload.")

    filepath: Optional[str] = Field(
        default=None,
        description="An optional filepath. Use this to explicitly mark the generated text as refering to one of the files in the context.",
    )

    def show_cli(self, **kwargs: Any) -> str:
        """A (short) CLI representation for the part."""
        filepath_str = f"## {self.filepath}\n" if self.filepath else ""
        return f"{filepath_str}{self.text}"

    def show(self, **kwargs: Any) -> str:
        """Comprehensive longform string representation for the part."""
        filepath_str = f"## {self.filepath}\n\n" if self.filepath else ""
        return f"""{filepath_str}{self.text}
"""


class ShellCommandPart(BaseModel):
    """Represents a shell command that will be run.
    The output of the command will either be used for further processing by the worker, sent to the coder backend, shown to the user, or stored in a file.
    By default, commands are run relative to the project root directory.
    Note to ghostworker and ghostcoder: Use this in the following situations:
        1. If the user directly requests a shell command to be run.
        2. If running a shell command is the easiest, most direct, or obvious way to fulfill the users request (e.g. If the user asks 'How much space do I have on my hard drive' - don't write a python script that calculates the space, just run 'df -h'
        3. If there was an error or failure that you are trying to recover from, and running a shell command will aid the recovery.
        4. If there was an error, failure, or you were unable to fulfill a user request with the current context, and running a shell command may give you the additional information you need, like running a test, checking wether a package is isntalled, or inspecting the git logs.
    """

    command: str = Field(
        description="The full shell command that will be run in a virtual terminal."
    )

    reason: str = Field(
        description="A short, informative statement that describes the reason and intent behind this shell command. This will be shown to the user and stored in the logs."
    )

    def show_cli(self, **kwargs: Any) -> str:
        return f"""## Shell command
{self.reason}

```bash
 $> {self.command}
```
        """

    def show(self, **kwargs: Any) -> str:
        return show_model(self)


class FilesLoadPart(BaseModel):
    """Represents a request to load files into context so that their contents become available to the coder LLM.
    Note to ghostcoder and ghostworker: Use this if the context information you have is not sufficient to fulfill the request.
    """

    filepaths: List[str] = Field(
        description="One or more  filepaths relative to the project root which will be loaded into context."
    )

    def show_cli(self, **kwargs: Any) -> str:
        return f"""## Load Files into Context
{" -> ".join(self.filepaths)}
"""

    def show(self, **kwargs: Any) -> str:
        return show_model(self)


class FilesUnloadPart(BaseModel):
    """Represents a request to unload one or more files from the current context.
    Unloading files reduces the amount of tokens sent to the coder LLM backend, which will improve performance and may save on token costs.
    Files should only be unloaded if they really are unnecessary to the current task.
    Note to ghostworker: Use this if there is a failure that may be recovered from by reducing token use or shrinking the context. Pick files to unload that are unlikely to be relevant to the current request.
    """

    filepaths: List[str] = Field(
        description="One or more filepaths relative to the project root that should be unloaded from the current context."
    )

    def show_cli(self, **kwargs: Any) -> str:
        return f"""## Unload files from context
{" <- ".join(self.filepaths)}
"""

    def show(self, **kwargs: Any) -> str:
        return show_model(self)


# this one has all the parts
type LLMResponsePart = CodeResponsePart | TextResponsePart | ShellCommandPart | FilesLoadPart | FilesUnloadPart | WorkerCoderRequestPart | EndInteractionPart

# watch out, a kCoderResponsePart is a subset of all available ResponseParts!
type CoderResponsePart = CodeResponsePart | TextResponsePart


class CoderResponse(BaseModel):
    """A response returned from the code generation backend.
    Coder responses are conceptual in nature and contain code and text. THey are not intended to be tool-calls etc.
    """

    contents: Sequence[CoderResponsePart] = Field(
        default_factory=lambda: [],
        description="A list of response parts returned by the backend coder LLM. This may contain code and/or text.",
    )

    def show_cli(self, **kwargs: Any) -> str:
        return "\n".join([part.show_cli(**kwargs) for part in self.contents])

    def show(self, **kwargs: Any) -> str:
        return "\n".join([part.show(**kwargs) for part in self.contents])


class WorkerCoderRequestPart(BaseModel):
    """Represents a request that should be send to the coder LLM backend.
        This allows the worker to communicate with the coder for e.g. clarification, refactoring, adjustment to the user prompt, or anything else that requires the coders capabilities.
    Note to ghostworker: Use this response part to indicate that you are bouncing a request, and delegating it to the coder LLM. Only do this if you think the other response parts are unable to fulfill the user's request, or if you think they are inadqeuate to recover from an error or failure.
    """

    text: str = Field(
        description="The plaintext prompt that will be sent to the coder LLM."
    )

    reason: str = Field(
        description="A short, informative message describing the reason for the request. This message will be displayed to the user and stored in the logs."
    )

    def show_cli(self) -> str:
        return f"""## Request from ghostworker to ghostcoder

```
        {show_model(self)}
```

"""

    def show(self) -> str:
        return show_model(self)


class EndInteractionPart(BaseModel):
    """Represents a signal to end the current interaction.
    Note to ghostcoder and ghostworker: Use this reponse part if a user request has been fulfilled and you deem the interaction to be completely finished.
    """

    reason: str = Field(description="The reason for ending the interaction.")

    def show_cli(self) -> str:
        return show_model(self)

    def show(self) -> str:
        return show_model(self)


# watch out, WorkerResponsePart is a subset of all available kResponsePart types, and it's not the same as a CoderResponsePartresponse
type WorkerResponsePart = ShellCommandPart | FilesLoadPart | FilesUnloadPart | WorkerCoderRequestPart | EndInteractionPart


class WorkerResponse(BaseModel):
    """A response returned from the worker backend.
    Worker responses serve to either enable or improve prompts to the coder backend, or implement responses from the coder backend. They may include tool calls like file reads, shell commands, or http requests, as well as follow-up prompts to the coder backend for clarification.
    """

    contents: List[WorkerResponsePart] = Field(
        description="The various response parts returned by the worker backend."
    )

    def show_cli(self) -> str:
        return "\n\n".join([part.show_cli() for part in self.contents])

    def show(self) -> str:
        return "\n\n".join([part.show() for part in self.contents])


class CoderInteractionHistoryItem(BaseModel):
    """A LLM coder response that was generated during a user interaction."""

    timestamp: str = Field(default_factory=timestamp_now_iso8601)

    context: ContextFiles = Field(
        description="The context that was current at the time the interaction was made."
    )

    backend: str = Field(
        description="The LLM backend that was used at the time the interaction was made. See ghostbox.definitions.LLMBackends."
    )

    model: str = Field(
        description="The particular LLM model that was used by the backend at the time the interaction was made."
    )

    response: CoderResponse = Field(
        description="The full response returned by the LLM. as part of this interaction."
    )

    git_commit_hash: Optional[str] = Field(
        default=None,
        description="The commit that was checked out at the time this interaction took place.",
    )

    applied_commit_hash: Optional[str] = Field(
        default = None,
        description = "Commit hash if this interaction actually altered git repo state with a commit. This is very different from git_commit_hash, which is always set in a repo under normal circumstances. This only applised if the interaction item lead to code that was commited."
    )    

    
    @model_validator(mode='before')
    @classmethod
    def set_git_commit_hash(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        """
        Populates the git_commit_hash if it's not already set and a project root
        is available from the context. This validator attempts to retrieve the
        current Git commit hash.
        """
        # FIXME: does not respect project.config.git_integration. Maybe add that to the context?
        # If git_commit_hash is already provided, or if context is missing, skip
        if values.get('git_commit_hash') is not None:
            return values
        
        # Ensure 'context' is present and has a 'root' attribute (which it should if it's a ContextFiles instance)
        if 'context' not in values or not isinstance(values['context'], ContextFiles):
            logger.debug("Skipping git_commit_hash validation: 'context' not a ContextFiles instance or missing.")
            return values

        project_root = values['context'].root
        
        # Attempt to get the current commit hash
        # git.get_current_commit_hash handles non-git repos gracefully by returning an error
        commit_hash_result = git.get_current_commit_hash(project_root)
        
        if commit_hash_result.is_ok() and commit_hash_result.value is not None:
            values['git_commit_hash'] = commit_hash_result.value
            logger.debug(f"UserInteractionHistoryItem validator set git_commit_hash to {commit_hash_result.value}.")
        elif commit_hash_result.is_err():
            logger.debug(f"UserInteractionHistoryItem validator could not get git commit hash for project root {project_root}: {commit_hash_result.error}")
        
        return values
    

    def show(self, **kwargs: Any) -> str:
        """Returns a human readable string representation of the item."""
        git_str = f"@[commit {self.git_commit_hash}]" if self.git_commit_hash is not None else ""
        applied_str = f"\n  !! This message created commit {self.applied_commit_hash} !!\n" if self.applied_commit_hash is not None else ""
        return f"""[{self.timestamp}]{git_str} {self.context.show_cli()}{applied_str}
  {self.model}@{self.backend} >
{        self.response.show(**kwargs)}
"""

    def to_chat_message(self) -> ghostbox.ChatMessage:
        """Turn this coder interaction into a ghostbox compatible chat message."""
        # we need to find actual text
        text_parts = []
        for content in self.response.contents:
            match content:
                case TextResponsePart() as text_response_part:
                    text_parts.append(text_response_part.text)
                case _:
                    # This is tricky. We mighth ave code parts here, or file edits, and we don't want to clutter up the history with now outdated, and high-token data.
                    # on the other hand we usually use chat messages like this to reconstruct a past interaction history for the user to resumse, and they probably want the same context they had originally.
                    # for now, we just turn the part into json and show it to the model. with a heading
                    text_parts.append(
                        quoted_if_nonempty(
                            text=json.dumps(content.model_dump()), heading=content.__class__.__name__
                        )
                    )

        # Note: context is omitted intentionally.
        return ghostbox.ChatMessage(role="assistant", content="\n\n".join(text_parts))


class UserInteractionHistoryItem(BaseModel):
    """Part of an AI/User interaction representing a user prompt."""

    timestamp: str = Field(default_factory=timestamp_now_iso8601)

    git_commit_hash: Optional[str] = Field(
        default=None,
        description="The commit that was at the head position at the time this interaction item was produced. None if no git repository is present or the hash could otherwise not be determined.",
    )

    applied_commit_hash: Optional[str] = Field(
        default = None,
        description = "Commit hash if this interaction actually altered git repo state with a commit. This is very different from git_commit_hash, which is always set in a repo under normal circumstances. This only applised if the interaction item lead to code that was commited."
    )    
    
    context: ContextFiles = Field(
        description="The context that was current at the time the interaction was made."
    )

    # FIXME: preamble should probably be removed here and tracked in some different way (on the entire chat history for example)
    preamble: str = Field(
        default="",
        description="The plaintext of the context that was added automatically before the user prompt. This is usually long and not present in every interaction",
    )

    prompt: str = Field(
        description="The actual user prompt. This includes only the plain text created directly by the user at the time of the interaction."
    )

    @model_validator(mode='before')
    @classmethod
    def set_git_commit_hash(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        """
        Populates the git_commit_hash if it's not already set and a project root
        is available from the context. This validator attempts to retrieve the
        current Git commit hash.
        """
        # FIXME: does not respect project.config.git_integration. Maybe add that to the context?
        # If git_commit_hash is already provided, or if context is missing, skip
        if values.get('git_commit_hash') is not None:
            return values
        
        # Ensure 'context' is present and has a 'root' attribute (which it should if it's a ContextFiles instance)
        if 'context' not in values or not isinstance(values['context'], ContextFiles):
            logger.debug("Skipping git_commit_hash validation: 'context' not a ContextFiles instance or missing.")
            return values

        project_root = values['context'].root
        
        # Attempt to get the current commit hash
        # git.get_current_commit_hash handles non-git repos gracefully by returning an error
        commit_hash_result = git.get_current_commit_hash(project_root)
        
        if commit_hash_result.is_ok() and commit_hash_result.value is not None:
            values['git_commit_hash'] = commit_hash_result.value
            logger.debug(f"UserInteractionHistoryItem validator set git_commit_hash to {commit_hash_result.value}.")
        elif commit_hash_result.is_err():
            logger.debug(f"UserInteractionHistoryItem validator could not get git commit hash for project root {project_root}: {commit_hash_result.error}")
        
        return values
    
    def show(self, **kwargs: Any) -> str:
        git_str = f"@[commit {self.git_commit_hash}]" if self.git_commit_hash is not None else ""
        applied_str = f"\n  !! This message created commit {self.applied_commit_hash} !!\n" if self.applied_commit_hash is not None else ""        
        return f"""[{self.timestamp}]{git_str} {self.context.show_cli()}{applied_str}
  user >
        {self.prompt}
"""

    def to_chat_message(self) -> ghostbox.ChatMessage:
        """Convert a user interaction item to a ghostbox chatmessage, allowing it to be injected into a ghostbox history."""
        # Note that we only include the prompt here, omitting e.g. context files and preamble. This is not a mistake, it is intentional.
        return ghostbox.ChatMessage(role="user", content=self.prompt)


type InteractionHistoryItem = UserInteractionHistoryItem | CoderInteractionHistoryItem


class InteractionHistory(BaseModel):
    """An interaction history represents 0 or more conversational turns between user and an LLM backend.
    This is usually saved to .ghostcode/interaction_history.json, where4 all interaction histories of a project are tracked.
    """

    unique_id: str = Field(
        default_factory=lambda: str(uuid4()),
        description="Randomly generated unique identified that can be used to retrieve the interaction history.",
    )

    tag: Optional[str] = Field(
        default_factory=lambda: make_mnemonic(max_length=3, min_length=2),
        description="An easy to remember, somewhat unique string that can be used to retrieve the conversation history. If any ambiguity is encountered, the unique_id is used as a fallback. Can also be set by users.",
    )

    title: str = Field(
        default="",
        description="A short phrase summarizing what the interaction was about. An interaction's title may change over time.",
    )

    contents: List[InteractionHistoryItem] = Field(
        default_factory=lambda: [],
        description="List of past interactions in this ghostcode project.",
    )

    summary: str = Field(
        default = "",
        description = "A brief summary of the entire interaction history. This may or may not be set and is often generated during idle time in the background."
    )

    branch_name: Optional[str] = Field(
        default = None,
        description = "The name of the git branch, if any, that was checked out at the time this interaction was created."
    )
        
    def empty(self) -> bool:
        """Returns true if the history is empty."""
        return self.contents == []

    def timestamps(self) -> Optional[Tuple[str, str]]:
        """Returns a pair of (earliest interaction, most recent itneraction), or None if no interactions present."""
        if self.empty():
            return None

        # rely on the fact that iso8601 timestamps equate chronological with lexicographical ordering
        timestamps = sorted([item.timestamp for item in self.contents])
        return (timestamps[0], timestamps[-1])

    def get_affected_git_commits(self) -> List[str]:
        """Returns a (non-duplicate containing) list of git commits that altered repo state and were part of this interaction."""
        return list(
            set(
                [
                    hash
                    for item in self.contents
                    if (hash := item.applied_commit_hash) is not None
                ]
            )
        )

    def show(self, include_code: bool = True, drop_last: int = 0) -> str:
        """Returns a human readable text representation of the history."""
        return "\n".join(
            [
                item.show(include_code=include_code)
                for item in self.contents[: len(self.contents) - drop_last]
            ]
        )

    def to_chat_messages(self) -> List[ghostbox.ChatMessage]:
        """Turn an InteractionHistory into a ghostbox chat message history compatible with ghostbox.set_history."""
        return [part.to_chat_message() for part in self.contents]
    

### Actions ###
# These are put on the action queue and sequentially resolved.
# Actions share some structure with ResponseParts but are also different.
# In general, the distinction between Actions and ResponseParts exists to alleviate the context burden on the LLM backend, both for memory constraints and performance degradation with large contexts. In particular:
# 1. Actions are more fine grained and low level. They contain things that are hard for LLMs (e.g. line numbers/character positions for file edits).
# 2. Actions have clearance and user confirmation logic associated with them. LLM ResponseParts do not (we can't keep an LLM from generating a part).
# 3. Actions can fail.
# 4. A ResponsePart may map to 0 or more actions.

class ActionDoNothing(BaseModel):
    """Action that represents a noop.
    Used mostly as an identity under composition in the 'Action' type."""

    clearance_required: ClassVar[ClearanceRequirement] = ClearanceRequirement.AUTOMATIC


class ActionHaltExecution(BaseModel):
    """Represents a signal to halt the execution of the action queue. This is put on the queue whenever execution has run into too many errors or has encoutnered a critical problem that can not be overcome."""

    clearance_required: ClassVar[ClearanceRequirement] = ClearanceRequirement.AUTOMATIC

    reason: str = Field(description="The reason for the halt signal in plain english.")

    announce: bool = Field(
        default=False,
        description="Wether to announce the reason for the halting to the user. During regular execution, this is usually unnecessary.",
    )


class ActionHandleCodeResponsePart(BaseModel):
    """This action represents a code response that was received by the coder LLM and needs to be handled.
    This is a thin wrapper around a CodeResponsePart. It may be handled in a variety of ways, the exact manner of which is usually determined by the worker LLM.
    """

    clearance_required: ClassVar[ClearanceRequirement] = ClearanceRequirement.AUTOMATIC

    content: CodeResponsePart = Field(
        description="The original code response part that was received from the coder LLM."
    )


class ActionFileCreate(BaseModel):
    """Represents the creation of a new file."""

    clearance_required: ClassVar[ClearanceRequirement] = ClearanceRequirement.CONFIRM
    filepath: str = Field(description="The path to the file that should be created.")

    content: str = Field(
        description="The contents that will be written to the newly created file."
    )


class ActionFileEdit(BaseModel):
    """Representation of a single replace operation in a file.
    This type is intended to be the end result of a worker file edit and the last step before the actual text is changed on disk.
    """

    clearance_required: ClassVar[ClearanceRequirement] = ClearanceRequirement.CONFIRM

    filepath: str = Field(description="The file that should be edited.")

    replace_pos_begin: int = Field(
        description="The character position in the file that marks the beginning of the text that should be replaced. Assume python-style indices."
    )

    replace_pos_end: int = Field(
        description="The character position in the file that marks the end of the text that should be replaced. Assume python-style indices."
    )

    insert_text: str = Field(
        description="The text that will be inserted as a replacement for the text between replace_pos_begin and replace_pos_end."
    )

    def get_original_info(self) -> Tuple[str, Tuple[int, int]]:
        """Returns < original code, < first line affected, last line affected> >.
        Returns < "", <0, 0>> on error.
        This is intended as a helper for user confirmation dialogs. Don't use this to make actual file edits/levenshtein calculations.
        """
        try:
            with open(self.filepath) as f:
                w = f.read()

            newlines_to_begin = len(
                [c for c in w[: self.replace_pos_begin] if c == "\n"]
            )
            newlines_to_end = len([c for c in w[: self.replace_pos_end] if c == "\n"])
            original_code = w[self.replace_pos_begin : self.replace_pos_end]
            return (original_code, (newlines_to_begin, newlines_to_end))
        except Exception as e:
            logger.exception(
                f"Couldn't open file for original info during edit action. Reason: {e}. Continuing."
            )
        return ("", (0, 0))


class ActionShellCommand(BaseModel):
    """Action for executing shell commands.
    This is one security nightmare, but I guess we are doing this!
    This type shares most of its structure with the ShellCommandResponsePart and so contains it in full.
    """

    clearance_required: ClassVar[ClearanceRequirement] = ClearanceRequirement.DANGEROUS

    content: ShellCommandPart = Field(
        description="The various parameters required to invoke subprocess.Popen. Stored in a Part object for convenience and reuse."
    )


# helper classes for waiting on shell commands


class ShellWaitResponsePartKeepWaiting(BaseModel):
    """A response part indicating that a currently executing shell process should be given more time to finish execution."""

    reason: str = Field(
        description="The reason that the shell command is likely to finish within a reasonable time, or the reason why it is advisable to keep waiting for the execution to finish a little longer."
    )

    time_to_wait_seconds: float = Field(
        description="How many seconds to wait for the process to finish. After the time has elapsed, another check will be made to see if execution of the shell command is likely to ever finish."
    )


class ShellWaitResponsePartKillProcess(BaseModel):
    """A response part indicating that the running shell process is unlikely to finish and should be killed."""

    reason: str = Field(description="Why the shell process is unlikely to finish.")


class ShellWaitResponsePartFinished(BaseModel):
    """A response part indicating that the shell process has finished execution."""

    reason: str = Field(
        description="Why it is reasonable to assume that the process has finished."
    )


type ShellWaitResponsePart = ShellWaitResponsePartKeepWaiting | ShellWaitResponsePartKillProcess | ShellWaitResponsePartFinished


class ShellWaitResponse(BaseModel):
    """Determines wether a shell command has finished, is still in progress and should be waited on further, or is unlikely to ever succeed and should be canceled."""

    content: ShellWaitResponsePart = Field(
        description="The response part that indicates how to proceed with the shell command."
    )


class ActionWaitOnShellCommand(BaseModel):
    """Wait for an executing shell command to finish, periodically checking the output to judge progress."""

    clearance_required: ClassVar[ClearanceRequirement] = ClearanceRequirement.AUTOMATIC

    original_command: ActionShellCommand = Field(
        description="The original shell command that was invoked is not being waited for."
    )


# These are a bunch of small classes for a ContextAlteration type
# I wish we had less verbose ways to do sum types but oh well


class ContextAlterationLoadFile(BaseModel):
    filepath: str


class ContextAlterationUnloadFile(BaseModel):
    filepath: str


class ContextAlterationFlagFile(BaseModel):
    # this is a placeholder and not used yet.
    flag: bool


type ContextAlteration = ContextAlterationLoadFile | ContextAlterationUnloadFile | ContextAlterationFlagFile


class ActionAlterContext(BaseModel):
    """Action to either load, unload, or otherwise mark files in the ghostcode context. Files that are not in context will not be sent to the coder LLM backend."""

    clearance_required: ClassVar[ClearanceRequirement] = ClearanceRequirement.INFORM

    context_alteration: ContextAlteration


class ActionQueryCoder(BaseModel):
    """Query the ghostcoder backend for something.
    The purpose of the  query is left intentionally broad. Executing this action will usually result in 1 or more ActionHandleResponsePart being pushed onto the action queue, unless the query fails, in which worker recovery is invoked pushed.
    """

    clearance_required: ClassVar[ClearanceRequirement] = ClearanceRequirement.INFORM

    prompt: str = Field(
        description="The full prompt that will be sent to the coder LLM backend. Facilitating the query will be done by ghostbox, which may do substitutions for special strings."
    )

    llm_response_profile: LLMResponseProfile = Field(
        default_factory=LLMResponseProfile,
        description="May constrain the types of responses generated by the coder, e.g. text-only.",
    )

    hidden: bool = Field(
        default=False,
        description="Hidden queries are not shown to the user. Unhidden queries will have their response parts displayed with the various show_cli methods.",
    )

    interaction_history_id: Optional[str] = Field(
        default=None,
        description="ID of an interaction history that is associated with this query. If provided, the response to the query will be appended to this history.",
    )


class ActionQueryWorker(BaseModel):
    """Query the ghostworker backend for something."""

    clearance_required: ClassVar[ClearanceRequirement] = ClearanceRequirement.INFORM

    prompt: str = Field(
        description="The full prompt that will be sent to the worker LLM backend. Facilitating the query will be done by ghostbox, which may do substitutions for special strings."
    )

    llm_response_profile: LLMResponseProfile = Field(
        default_factory=LLMResponseProfile,
        description="May constrain the types of responses generated by the worker, e.g. text-only.",
    )

    hidden: bool = Field(
        default=True,
        description="Hidden queries are not shown to the user. Unhidden queries will have their response parts displayed with the various show_cli methods.",
    )

    interaction_history_id: Optional[str] = Field(
        default=None,
        description="ID of an interaction history that is associated with this query. If provided, the response to the query will be appended to this history.",
    )


class UserPromptClassification(BaseModel):
    """Represents wether a user prompt is to be classified as being a 'worker' responsibility, or a 'coder' responsibility."""

    request: str = Field(
        description="The original user prompt that was classified as either 'worker' or 'coder'."
    )

    classification: AIAgent = Field(
        description="The category that the user request belongs to."
    )

    reason: str = Field(
        description="The reason the prompt should be classified this way."
    )


class ActionRouteRequest(BaseModel):
    """Decide on wether to route a user request to the ghostcoder or ghostworker."""

    clearance_required: ClassVar[ClearanceRequirement] = ClearanceRequirement.AUTOMATIC

    prompt: str = Field(description="The user prompt to route.")

    llm_response_profile: LLMResponseProfile = Field(
        default_factory=LLMResponseProfile,
        description="The response profile that will be passed through to the route receiving LLM.",
    )

    hidden: bool = Field(
        default=False,
        description="Hidden queries are not shown to the user. Unhidden queries will have their response parts displayed with the various show_cli methods.",
    )

    interaction_history_id: Optional[str] = Field(
        default=None,
        description="ID of an interaction history that is associated with this query. If provided, the response to the query will be appended to this history.",
    )


type Action = ActionHandleCodeResponsePart | ActionFileCreate | ActionFileEdit | ActionDoNothing | ActionHaltExecution | ActionShellCommand | ActionWaitOnShellCommand | ActionAlterContext | ActionQueryCoder | ActionQueryCoder | ActionQueryWorker | ActionRouteRequest
type QueryAction = ActionQueryCoder | ActionQueryWorker


def action_show_short(action: Action) -> str:
    """Gives a short representation of an action.
    By default, this function returns the actions type name with the conventional Action prefix removed. Some actions may give additional information, e.g. ActionFileEdit becomes "FileEdit(types.py)".
    This short representation is used at various points in the CLI interface, such as when asking the user for permission to perform an action.
    """

    canonical_name = action.__class__.__name__
    if canonical_name.startswith("Action"):
        name = canonical_name[len("Action") :]
    else:
        name = canonical_name

    # customize for some actions
    # this is the kind of thing you could endlessly overcomplicate with an ABC interface when a simple match case will do just fine
    match action:
        case ActionQueryCoder() as query_coder_action:
            return ""
        case ActionQueryWorker():
            return ""
        case ActionShellCommand() as shell_command_action:
            # don't want to import shlex this is good enough
            args = shell_command_action.content.command.split(" ")
            executable_str = args[0] if args else ""
            return f"{name}({executable_str})"
        case ActionFileEdit() as file_edit_action:
            return f"{name}({file_edit_action.filepath})"
        case ActionFileCreate() as file_create_action:
            return f"{name}({file_create_action.filepath})"
        case _:
            return name


def action_show_user_message(
    action: Action, delimiter_color: Optional[Color256] = None
) -> str:
    """Display a user message for the action, which is printed when the action is popped off the action queue."""
    if delimiter_color is None:
        delimiter = " >>= "
    else:
        delimiter = f" {colored(">>=", delimiter_color)} "

    match action:
        case _:
            result = action_show_short(action)
    return result + delimiter


def action_show_confirmation_data(
    action: Action, first_showing: bool = True, abridge: Optional[int] = None
) -> str:
    """Returns a string representation to show on the command line during a dialog that confirms the execution of the action.
    The abridge parameters can limit long parts of action's attributes to a certain character limit.
    """

    # for many we just default as they don't need confirm often
    # but some (like file kedit) get special care
    match action:
        case ActionFileEdit() as file_edit_action:
            if first_showing:
                return quoted_if_nonempty(
                    text=file_edit_action.insert_text,
                    heading=file_edit_action.filepath + " (new code)",
                    heading_level=1,
                )
            else:
                # give much more information
                # FIXME: show diff
                (original_code, (first_line, last_line)) = (
                    file_edit_action.get_original_info()
                )
                w = f"# {file_edit_action.filepath}\nlines {first_line} - {last_line}\n\n"
                w += quoted_if_nonempty(
                    text=original_code, heading="Original Code", heading_level=2
                )
                w += quoted_if_nonempty(
                    text=file_edit_action.insert_text,
                    heading="New Code",
                    heading_level=2,
                )

                return w
        case _:
            return show_model_nt(
                action, heading=action.__class__.__name__, abridge=abridge
            )


class ActionResultOk(BaseModel):
    """Represents a successfully executed action."""

    # maybe add timing information?
    success_message: Optional[str] = Field(
        default=None,
        description="Optional message that describes the successfully completed action or its result.",
    )


class ActionResultFailure(BaseModel):
    """Represents an action that failed to execute for some reason."""

    original_action: Action = Field(
        description="The original action that failed to execute."
    )

    error_messages: List[str] = Field(
        default_factory=lambda: [],
        description="Any error messages, logs, failure rports, or other technical indicators of failure that are associated with this action.",
    )

    failure_reason: str = Field(
        description="The reason or problem that kept the action from executing either partially or completely, in plain english."
    )


class ActionResultMoreActions(BaseModel):
    """Result that represents that more actions need to be executed, along with action objects representing the required actions. Crucially, actions in the MoreActions result are pushed to the front of the action queue."""

    actions: List[Action] = Field(
        default_factory=lambda: [],
        description="The queue of actions that will need to be executed.",
    )


type ActionResult = ActionResultOk | ActionResultFailure | ActionResultMoreActions


class Project(BaseModel):
    """Basic context for a ghostcode project.
    By convention, all Fields of this type are found in the .ghostcode directory at the project root, with their Field names also being the filename.
    """

    config: ProjectConfig = Field(
        default_factory=ProjectConfig, description="Project wide configuration options."
    )

    worker_llm_config: Dict[str, Any] = Field(
        default_factory=dict,
        description="Configuration options sent to ghostbox for the worker LLM. This is a json file stored in .ghostcode/worker_llm_config.json.",
    )

    coder_llm_config: Dict[str, Any] = Field(
        default_factory=dict,
        description="Configuration options sent to ghostbox for the coder LLM. This is a json file stored in .ghostcode/coder_llm_config.json.",
    )

    context_files: ContextFiles

    directory_file: str = Field(
        default="",
        description="A directory file is an automatically generated markdown file intended to keep LLMs from introducing changes that break the project. It usually contains a project overview and important information about the project, such as module dependencies, architectural limitations, technology choices, and much more. Stored in .ghostcode/directory_file.md.",
    )
    
    project_metadata: Optional[ProjectMetadata] = Field(
        default_factory=lambda: ProjectMetadata(**DEFAULT_PROJECT_METADATA)
    )

    interactions: List[InteractionHistory] = Field(
        default_factory=lambda: [],
        description="List of interactions that have occurred in the past.",
    )

    # --- File names within the .ghostcode directory ---
    _GHOSTCODE_DIR: ClassVar[str] = ".ghostcode"
    _WORKER_CHARACTER_FOLDER: ClassVar[str] = "worker"
    _WORKER_CONFIG_FILE: ClassVar[str] = "worker/config.json"
    _CODER_CHARACTER_FOLDER: ClassVar[str] = "coder"
    _CODER_CONFIG_FILE: ClassVar[str] = "coder/config.json"
    _CONTEXT_FILES_FILE: ClassVar[str] = "context_files"  # Plaintext list of filepaths
    _CONTEXT_FILES_CONFIGS_FILE: ClassVar[str] = "context_files_config.json"  # json dict of associated configs
    _DIRECTORY_FILE: ClassVar[str] = "directory_file.md"  # Plaintext markdown
    _PROJECT_METADATA_FILE: ClassVar[str] = "project_metadata.yaml"  # YAML format
    _PROJECT_CONFIG_FILE: ClassVar[str] = "config.yaml"  # YAML format
    _INTERACTION_HISTORY_FILE: ClassVar[str] = "interaction_history.json"
    _CURRENT_INTERACTION_HISTORY_FILE: ClassVar[str] = "current.json"
    _CURRENT_INTERACTION_PLAINTEXT_FILE: ClassVar[str] = "current.txt"
    _LOG_FILE: ClassVar[str] = "log.txt"
    _STYLE_FILE: ClassVar[str] = "style.md"
    
    _WORKER_SYSTEM_MSG: ClassVar[str] = (
        "You are GhostWorker, a helpful AI assistant focused on executing specific, local programming tasks. Your primary role is to interact with the file system, run shell commands, and perform other environment-specific operations as instructed by GhostCoder. Be concise, precise, and report results clearly. Do not engage in high-level planning or code generation unless explicitly asked to generate a small snippet for a tool."
    )
    _CODER_SYSTEM_MSG: ClassVar[str] = (
        "You are GhostCoder, a highly intelligent and experienced AI programmer. Your role is to understand complex programming tasks, devise high-level plans, generate code, and review existing code. You will delegate specific environment interactions (like reading/writing files or running commands) to GhostWorker. Focus on architectural decisions, code quality, and problem-solving. When delegating, provide clear and unambiguous instructions to GhostWorker."
    )

    @staticmethod
    def _get_ghostcode_path(root: str) -> str:
        """Helper to get the full path to the .ghostcode directory."""
        return os.path.join(root, Project._GHOSTCODE_DIR)

    @staticmethod
    def find_project_root(start_path: str = ".") -> Optional[str]:
        """
        Traverses up the directory tree from start_path to find the .ghostcode directory.

        Args:
            start_path (str): The directory to start searching from.

        Returns:
            Optional[str]: The absolute path to the project root (parent of .ghostcode), or None if not found.
        """
        current_path = os.path.abspath(start_path)
        while True:
            ghostcode_path = os.path.join(current_path, Project._GHOSTCODE_DIR)
            if os.path.isdir(ghostcode_path):
                logger.debug(
                    f"Found .ghostcode directory at {ghostcode_path}. Project root is {current_path}"
                )
                return current_path

            parent_path = os.path.dirname(current_path)
            if parent_path == current_path:  # Reached filesystem root
                logger.debug(
                    f"No .ghostcode directory found up to filesystem root from {start_path}"
                )
                return None
            current_path = parent_path

    @staticmethod
    def init(root: str) -> None:
        """
        Sets up a .ghostcode directory in the given root directory with all the necessary default files.

        Args:
            root (str): The root directory of the project where .ghostcode should be created.
        """
        ghostcode_dir = Project._get_ghostcode_path(root)
        logger.info(f"Initializing .ghostcode directory at: {ghostcode_dir}")

        try:
            os.makedirs(ghostcode_dir, exist_ok=True)
            logger.info(f"Ensured directory {ghostcode_dir} exists.")

            # Create .ghostcode/worker and .ghostcode/coder directories
            worker_char_dir = os.path.join(ghostcode_dir, "worker")
            coder_char_dir = os.path.join(ghostcode_dir, "coder")
            os.makedirs(worker_char_dir, exist_ok=True)
            os.makedirs(coder_char_dir, exist_ok=True)
            logger.info(f"Created worker character directory at {worker_char_dir}")
            logger.info(f"Created coder character directory at {coder_char_dir}")

            # Create system_msg files within character directories
            worker_system_msg_path = os.path.join(worker_char_dir, "system_msg")
            with open(worker_system_msg_path, "w") as f:
                f.write(Project._WORKER_SYSTEM_MSG)
            logger.info(f"Created worker system_msg at {worker_system_msg_path}")

            coder_system_msg_path = os.path.join(coder_char_dir, "system_msg")
            with open(coder_system_msg_path, "w") as f:
                f.write(Project._CODER_SYSTEM_MSG)
            logger.info(f"Created coder system_msg at {coder_system_msg_path}")

            # 1. Create worker/config.json
            worker_config_path = os.path.join(
                ghostcode_dir, Project._WORKER_CONFIG_FILE
            )
            with open(worker_config_path, "w") as f:
                json.dump(DEFAULT_WORKER_LLM_CONFIG, f, indent=4)
            logger.info(f"Created default worker LLM config at {worker_config_path}")

            # 2. Create coder/config.json
            coder_config_path = os.path.join(ghostcode_dir, Project._CODER_CONFIG_FILE)
            with open(coder_config_path, "w") as f:
                json.dump(DEFAULT_CODER_LLM_CONFIG, f, indent=4)
            logger.info(f"Created default coder LLM config at {coder_config_path}")

            # 3. Create context_files (plaintext, initially empty)
            context_files_path = os.path.join(
                ghostcode_dir, Project._CONTEXT_FILES_FILE
            )
            with open(context_files_path, "w") as f:
                f.write("")  # Empty file
            logger.info(f"Created empty context files list at {context_files_path}")

            # 4. Create directory_file.md (plaintext, initially empty)
            directory_file_path = os.path.join(ghostcode_dir, Project._DIRECTORY_FILE)
            with open(directory_file_path, "w") as f:
                f.write("")  # Empty file
            logger.info(f"Created empty directory file at {directory_file_path}")

            # 5. Create project_metadata.yaml (YAML, with default metadata)
            project_metadata_path = os.path.join(
                ghostcode_dir, Project._PROJECT_METADATA_FILE
            )
            with open(project_metadata_path, "w") as f:
                yaml.dump(
                    DEFAULT_PROJECT_METADATA,
                    f,
                    indent=2,
                    sort_keys=False,
                    default_flow_style=False,
                    Dumper=PydanticEnumDumper,
                )
            logger.info(f"Created default project metadata at {project_metadata_path}")

            # 5.1 Create empty style.md
            style_file_path = os.path.join(
                ghostcode_dir, Project._STYLE_FILE
            )
            default_style_file = ""
            with open(style_file_path, "w") as f:
                f.write(default_style_file)
            logger.info(f"Wrote default empty style file {style_file_path}")
            
            # 6. Create config.yaml (YAML, with default ProjectConfig) 
            project_config_path = os.path.join(
                ghostcode_dir, Project._PROJECT_CONFIG_FILE
            )
            default_project_config = ProjectConfig()
            with open(project_config_path, "w") as f:
                yaml.dump(
                    default_project_config.model_dump(mode="json"),
                    f,
                    Dumper=PydanticEnumDumper,
                    default_flow_style=False,
                    indent=2,
                    sort_keys=False,
                )
            logger.info(f"Created default project config at {project_config_path}")

            # 7. Create interaction_history.json (JSON, initially empty)
            interaction_history_path = os.path.join(
                ghostcode_dir, Project._INTERACTION_HISTORY_FILE
            )
            with open(interaction_history_path, "w") as f:
                json.dump([], f, indent=4)
            logger.info(
                f"Created empty interaction history at {interaction_history_path}"
            )

            logger.info(f"Ghostcode project initialized successfully in {root}.")

        except OSError as e:
            logger.error(
                f"Failed to create .ghostcode directory or files in {root}: {e}"
            )
            raise
        except Exception as e:
            logger.error(
                f"An unexpected error occurred during initialization in {root}: {e}",
                exc_info=True,
            )
            raise

    @staticmethod
    def from_root(root: str) -> "Project":
        """
        Looks for a .ghostcode folder in the given root directory, loads all the necessary files,
        constructs the respective types, and then returns a new Project instance.

        Args:
            root (str): The root directory of the project.

        Returns:
            Project: A new Project instance populated with data from the .ghostcode directory.

        Raises:
            FileNotFoundError: If the .ghostcode directory does not exist.
        """
        ghostcode_dir = Project._get_ghostcode_path(root)
        logger.info(f"Loading ghostcode project from: {ghostcode_dir}")

        if not os.path.isdir(ghostcode_dir):
            logger.error(
                f"'.ghostcode' directory not found at {ghostcode_dir}. Please initialize the project first using `ghostcode init '{root}'`."
            )
            raise FileNotFoundError(
                f"'.ghostcode' directory not found at {ghostcode_dir}"
            )

        worker_llm_config = {}
        coder_llm_config = {}
        context_files = ContextFiles(root=root)
        directory_file_content = ""
        project_metadata = ProjectMetadata(**DEFAULT_PROJECT_METADATA)
        project_config = ProjectConfig()  # Initialize with defaults
        interactions = []

        # 1. Load worker_llm_config.json (now from worker/config.json)
        worker_config_path = os.path.join(ghostcode_dir, Project._WORKER_CONFIG_FILE)
        try:
            with open(worker_config_path, "r") as f:
                worker_llm_config = json.load(f)
            logger.debug(f"Loaded worker LLM config from {worker_config_path}")
        except FileNotFoundError:
            logger.warning(
                f"Worker LLM config file not found at {worker_config_path}. Using default configuration."
            )
            worker_llm_config = DEFAULT_WORKER_LLM_CONFIG
        except json.JSONDecodeError as e:
            logger.error(
                f"Error decoding JSON from {worker_config_path}: {e}. Using default configuration.",
                exc_info=True,
            )
            worker_llm_config = DEFAULT_WORKER_LLM_CONFIG
        except Exception as e:
            logger.error(
                f"An unexpected error occurred loading {worker_config_path}: {e}. Using default configuration.",
                exc_info=True,
            )
            worker_llm_config = DEFAULT_WORKER_LLM_CONFIG

        # 2. Load coder_llm_config.json (now from coder/config.json)
        coder_config_path = os.path.join(ghostcode_dir, Project._CODER_CONFIG_FILE)
        try:
            with open(coder_config_path, "r") as f:
                coder_llm_config = json.load(f)
            logger.debug(f"Loaded coder LLM config from {coder_config_path}")
        except FileNotFoundError:
            logger.warning(
                f"Coder LLM config file not found at {coder_config_path}. Using default configuration."
            )
            coder_llm_config = DEFAULT_CODER_LLM_CONFIG
        except json.JSONDecodeError as e:
            logger.error(
                f"Error decoding JSON from {coder_config_path}: {e}. Using default configuration.",
                exc_info=True,
            )
            coder_llm_config = DEFAULT_CODER_LLM_CONFIG
        except Exception as e:
            logger.error(
                f"An unexpected error occurred loading {coder_config_path}: {e}. Using default configuration.",
                exc_info=True,
            )
            coder_llm_config = DEFAULT_CODER_LLM_CONFIG

        # 3. Load context_files
        context_files_path = os.path.join(ghostcode_dir, Project._CONTEXT_FILES_FILE)
        context_files_configs_path = os.path.join(ghostcode_dir, Project._CONTEXT_FILES_CONFIGS_FILE)        
        try:
            with open(context_files_path, "r") as f:
                content = f.read()
                # context files are always relative to the root
                context_files = ContextFiles.from_plaintext(content, root=root)
            logger.debug(f"Loaded context files from {context_files_path}")
        except FileNotFoundError:
            logger.warning(
                f"Context files list not found at {context_files_path}. Using empty list."
            )
            context_files = ContextFiles(data=[], root=root)
        except Exception as e:
            logger.error(
                f"An unexpected error occurred loading {context_files_path}: {e}. Using empty list.",
                exc_info=True,
            )
            context_files = ContextFiles(data=[], root=root)

        try:
            with open(context_files_configs_path, "r") as f:
                config_dict = json.load(f)

            for filepath, config in config_dict.items():
                context_files.set_config(filepath, ContextFileConfig(**config))

        except FileNotFoundError:
            logger.warning(f"Context file configs file not found at {context_files_configs_path}.")
        except Exception as e:
            logger.exception(f"Unknown exception while loading configs for context files from {context_files_configs_path}. Reason: {e}")

                
        # 4. Load directory_file.md
        directory_file_path = os.path.join(ghostcode_dir, Project._DIRECTORY_FILE)
        try:
            with open(directory_file_path, "r") as f:
                directory_file_content = f.read()
            logger.debug(f"Loaded directory file from {directory_file_path}")
        except FileNotFoundError:
            logger.warning(
                f"Directory file not found at {directory_file_path}. Using empty content."
            )
            directory_file_content = ""
        except Exception as e:
            logger.error(
                f"An unexpected error occurred loading {directory_file_path}: {e}. Using empty content.",
                exc_info=True,
            )
            directory_file_content = ""

        # 5. Load project_metadata.yaml
        project_metadata_path = os.path.join(
            ghostcode_dir, Project._PROJECT_METADATA_FILE
        )
        try:
            with open(project_metadata_path, "r") as f:
                metadata_dict = yaml.safe_load(f)
                if metadata_dict:
                    project_metadata = ProjectMetadata(**metadata_dict)
                else:
                    logger.warning(
                        f"Project metadata file {project_metadata_path} is empty. Using default metadata."
                    )
                    project_metadata = ProjectMetadata(**DEFAULT_PROJECT_METADATA)
            logger.debug(f"Loaded project metadata from {project_metadata_path}")
        except FileNotFoundError:
            logger.warning(
                f"Project metadata file not found at {project_metadata_path}. Using default metadata."
            )
            project_metadata = ProjectMetadata(**DEFAULT_PROJECT_METADATA)
        except yaml.YAMLError as e:
            logger.error(
                f"Error decoding YAML from {project_metadata_path}: {e}. Using default metadata.",
                exc_info=True,
            )
            project_metadata = ProjectMetadata(**DEFAULT_PROJECT_METADATA)
        except Exception as e:
            logger.error(
                f"An unexpected error occurred loading {project_metadata_path}: {e}. Using default metadata.",
                exc_info=True,
            )
            project_metadata = ProjectMetadata(**DEFAULT_PROJECT_METADATA)

        # 6. Load config.yaml
        project_config_path = os.path.join(ghostcode_dir, Project._PROJECT_CONFIG_FILE)
        try:
            with open(project_config_path, "r") as f:
                config_dict = yaml.safe_load(f)
                if config_dict:
                    # Pydantic will handle validation and conversion for ProjectConfig
                    project_config = ProjectConfig(**config_dict)
                else:
                    logger.warning(
                        f"Project config file {project_config_path} is empty. Using default ProjectConfig."
                    )
                    project_config = ProjectConfig()
            logger.debug(f"Loaded project config from {project_config_path}")
        except FileNotFoundError:
            logger.warning(
                f"Project config file not found at {project_config_path}. Using default ProjectConfig."
            )
            project_config = ProjectConfig()
        except yaml.YAMLError as e:
            logger.error(
                f"Error decoding YAML from {project_config_path}: {e}. Using default ProjectConfig.",
                exc_info=True,
            )
            project_config = ProjectConfig()
        except Exception as e:
            logger.error(
                f"An unexpected error occurred loading {project_config_path}: {e}. Using default ProjectConfig.",
                exc_info=True,
            )
            project_config = ProjectConfig()

        # 7. Load interaction_history.json
        interaction_history_path = os.path.join(
            ghostcode_dir, Project._INTERACTION_HISTORY_FILE
        )
        try:
            with open(interaction_history_path, "r") as f:
                history_data = json.load(f)
                if history_data:
                    interactions = [
                        InteractionHistory(**history_item)
                        for history_item in history_data
                    ]
                else:
                    logger.warning(
                        f"Interaction history file {interaction_history_path} is empty. Using default InteractionHistory."
                    )
                    interactions = []
            logger.debug(f"Loaded interaction history from {interaction_history_path}")
        except FileNotFoundError:
            logger.warning(
                f"Interaction history file not found at {interaction_history_path}. Using empty history."
            )
            interactions = []
        except json.JSONDecodeError as e:
            logger.error(
                f"Error decoding JSON from {interaction_history_path}: {e}. Using empty history.",
                exc_info=True,
            )
            interactions = []
        except Exception as e:
            logger.error(
                f"An unexpected error occurred loading {interaction_history_path}: {e}. Using empty history.",
                exc_info=True,
            )
            interactions = []

        logger.info(f"Ghostcode project loaded successfully from {root}.")
        return Project(
            config=project_config,  # Pass the loaded config
            worker_llm_config=worker_llm_config,
            coder_llm_config=coder_llm_config,
            context_files=context_files,
            directory_file=directory_file_content,
            project_metadata=project_metadata,
            interactions=interactions,
        )

    def save_project_metadata(self) -> None:
        """Saves the current project metadata to the ghostcode directory."""
        root = self.find_project_root()
        if root is None:
            logger.error(f"Couldn't find project root while trying to save metadata.")
            return
        ghostcode_dir = Project._get_ghostcode_path(root)
        project_metadata_path = os.path.join(
            ghostcode_dir, Project._PROJECT_METADATA_FILE
        )
        if self.project_metadata:
            try:
                with open(project_metadata_path, "w") as f:
                    yaml.dump(
                        self.project_metadata.model_dump(mode="json"),
                        f,
                        Dumper=PydanticEnumDumper,
                        indent=2,
                        sort_keys=False,
                        default_flow_style=False,
                    )
                logger.debug(f"Saved project metadata to {project_metadata_path}")
            except Exception as e:
                logger.error(
                    f"Failed to save project metadata to {project_metadata_path}: {e}",
                    exc_info=True,
                )
                raise
        else:
            logger.warning(f"No project metadata to save for {root}.")

    def save_context_files(self, root: str) -> None:
        ghostcode_dir = Project._get_ghostcode_path(root)
        
        context_files_path = os.path.join(ghostcode_dir, Project._CONTEXT_FILES_FILE)
        context_files_configs_path = os.path.join(ghostcode_dir, Project._CONTEXT_FILES_CONFIGS_FILE)                
        try:
            # Sort context files alphabetically by filepath before saving
            self.context_files.data.sort(key=lambda cf: cf.filepath)
            with open(context_files_path, "w") as f:
                f.write(self.context_files.to_plaintext())
            logger.debug(f"Saved context files to {context_files_path}")
            config_dict = { cf.filepath: cf.config.model_dump() for cf in self.context_files.data}
            with open(context_files_configs_path, "w") as f:
                json.dump(config_dict, f)
            logger.info(f"Saved context file configs to {context_files_configs_path}.")
        except Exception as e:
            logger.error(
                f"Failed to save context files to {context_files_path}: {e}",
                exc_info=True,
            )
            raise
    
    def save_to_root(self, root: str) -> None:
        """
        Serializes all the contained types and saves them to the .ghostcode folder
        within the given root directory.

        Args:
            root (str): The root directory of the project.
        """
        ghostcode_dir = Project._get_ghostcode_path(root)
        logger.info(f"Saving ghostcode project to: {ghostcode_dir}")

        if not os.path.isdir(ghostcode_dir):
            logger.warning(
                f"'.ghostcode' directory not found at {ghostcode_dir}. Attempting to create it."
            )
            try:
                os.makedirs(ghostcode_dir)
            except OSError as e:
                logger.error(
                    f"Failed to create .ghostcode directory at {ghostcode_dir}: {e}"
                )
                raise

        # Ensure worker and coder character directories exist before saving their configs
        worker_char_dir = os.path.join(ghostcode_dir, "worker")
        coder_char_dir = os.path.join(ghostcode_dir, "coder")
        os.makedirs(worker_char_dir, exist_ok=True)
        os.makedirs(coder_char_dir, exist_ok=True)

        # 1. Save worker_llm_config.json (now to worker/config.json)
        worker_config_path = os.path.join(ghostcode_dir, Project._WORKER_CONFIG_FILE)
        try:
            with open(worker_config_path, "w") as f:
                json.dump(self.worker_llm_config, f, indent=4)
            logger.debug(f"Saved worker LLM config to {worker_config_path}")
        except Exception as e:
            logger.error(
                f"Failed to save worker LLM config to {worker_config_path}: {e}",
                exc_info=True,
            )
            raise

        # 2. Save coder_llm_config.json (now to coder/config.json)
        coder_config_path = os.path.join(ghostcode_dir, Project._CODER_CONFIG_FILE)
        try:
            with open(coder_config_path, "w") as f:
                json.dump(self.coder_llm_config, f, indent=4)
            logger.debug(f"Saved coder LLM config to {coder_config_path}")
        except Exception as e:
            logger.error(
                f"Failed to save coder LLM config to {coder_config_path}: {e}",
                exc_info=True,
            )
            raise

        # 3. Save context_files
        self.save_context_files(root)
        
        # 4. Save directory_file.md
        directory_file_path = os.path.join(ghostcode_dir, Project._DIRECTORY_FILE)
        try:
            with open(directory_file_path, "w") as f:
                f.write(self.directory_file)
            logger.debug(f"Saved directory file to {directory_file_path}")
        except Exception as e:
            logger.error(
                f"Failed to save directory file to {directory_file_path}: {e}",
                exc_info=True,
            )
            raise

        # 5. Save project_metadata.yaml
        # we used to do this but it would overwrite user data, so currently we don't save it
        
        # 6. Save config.yaml
        project_config_path = os.path.join(ghostcode_dir, Project._PROJECT_CONFIG_FILE)
        try:
            with open(project_config_path, "w") as f:
                yaml.dump(
                    self.config.model_dump(mode="json"),
                    f,
                    Dumper=PydanticEnumDumper,
                    indent=2,
                    default_flow_style=False,
                    sort_keys=False,
                )
            logger.debug(f"Saved project config to {project_config_path}")
        except Exception as e:
            logger.error(
                f"Failed to save project config to {project_config_path}: {e}",
                exc_info=True,
            )
            raise

        # 7. Save interaction_history.json
        interaction_history_path = os.path.join(
            ghostcode_dir, Project._INTERACTION_HISTORY_FILE
        )
        try:
            with open(interaction_history_path, "w") as f:
                json.dump(
                    [history.model_dump() for history in self.interactions], f, indent=4
                )
            logger.debug(f"Saved interaction history to {interaction_history_path}")
        except Exception as e:
            logger.error(
                f"Failed to save interaction history to {interaction_history_path}: {e}",
                exc_info=True,
            )
            raise

        logger.info(f"Ghostcode project saved successfully to {root}.")

    def new_interaction_history(self) -> InteractionHistory:
        """Create a new InteractionHistory and return a reference to it."""
        root = self.find_project_root()
        branch_name_gr = git.get_current_branch(root if root is not None else "")
        interaction_history = InteractionHistory(branch_name = branch_name_gr.value) 
        self.interactions.append(interaction_history)
        return interaction_history

    def get_interaction_history(
        self, unique_id: Optional[str] = None, tag: Optional[str] = None
    ) -> Optional[InteractionHistory]:
        """Returns an InteractionHistory if it can be retrieved by either ID or tag, None otherwise. If both tag and ID are supplied, the ID is prefered."""
        if unique_id is None and tag is None:
            logger.warning("Tried to get interaction history without ID or tag.")
            return None

        # we start at the back because imo recent interactions are more likely to be retrieved
        for interaction_history in reversed(self.interactions):
            if unique_id is not None:
                if interaction_history.unique_id == unique_id:
                    return interaction_history

            if interaction_history.tag == tag:
                return interaction_history
        return None

    def append_interaction_history_item(
        self,
        unique_id: Optional[str] = None,
        item: Optional[InteractionHistoryItem] = None,
    ) -> None:
        """Appends a given interaction history item to an existing interaction history with a specified unique_id.
        Raises index error if the interaction history with that unique_id is not found.
        """
        if unique_id is None or item is None:
            raise RuntimeError(
                f"Need all arguments supplied in call to append_interaction_history_item."
            )

        if (
            interaction_history := self.get_interaction_history(unique_id=unique_id)
        ) is None:
            logger.error(
                msg := f"Could not append to interaction history with id: {unique_id}: ID not found."
            )
            raise IndexError(msg)

        interaction_history.contents.append(item)

    def get_style(self) -> str:
        """Return contents of the style file. If the file is not found or unreadable, returns an empty string."""
        try:
            if (root := self.find_project_root()) is None:
                logger.warning(f"Can't find project root while trying to get style file path.")
                return ""
            
            style_file_path = os.path.join(self._get_ghostcode_path(root), self._STYLE_FILE)

            with open(style_file_path, "r") as f:
                return f.read()
        except Exception as e:
            # this is not a big deal
            logger.warning(f"CouldN't open style.md file. Reason: {e}")
        return ""

class CosmeticProgramState(Enum):
    """A vague indicator of program state. This is used to e.g. color certain outputs in the interface. Do not use this to query program state programmatically."""

    IDLE = 0
    WORKING = 10
    PROBLEM = 20
    CRITICAL_PROBLEM = 30

    def to_color(self) -> Color256:
        match self:
            case CosmeticProgramState.IDLE:
                return Color256.GREEN
            case CosmeticProgramState.WORKING:
                return Color256.ORANGE
            case CosmeticProgramState.PROBLEM:
                return Color256.DARK_MAGENTA
            case CosmeticProgramState.CRITICAL_PROBLEM:
                return Color256.RED
            case _:
                return Color256.GRAY_MEDIUM


@dataclass
class Program:
    """Holds program state for the main function.
    This instance is passed to command run methods."""

    project_root: Optional[str]
    project: Optional[Project]
    worker_box: Ghostbox
    coder_box: Ghostbox

    tty: shell.VirtualTerminal = field(
        default_factory=lambda: shell.VirtualTerminal(),
    )

    sound_manager: SoundManager = field(
        init = False
    )

    # holds the actions that are processes during single interactions. FIFO style
    action_queue: List[Action] = field(
        default_factory=lambda: [],
    )

    # flag indicating wether the user wants all actions in the queue to be auto-confirmed during a single interaction.
    action_queue_yolo: bool = False

    user_config: UserConfig = field(
        default_factory=UserConfig,
    )

    # holds the tag of the last message that was printed, useful for internal print logic
    last_print_tag: Optional[str] = None
    # always holds the last printed text, or empty string
    last_print_text: str = ""

    # used to color the interface and cannot be relied on except for cosmetics
    cosmetic_state: CosmeticProgramState = field(
        default_factory=lambda: CosmeticProgramState.IDLE
    )

    _DEBUG_DIR: ClassVar[str] = ".ghostcode/debug"

    def __post_init__(self) -> None:
        sound_dir = ghostcode.get_ghostcode_data("sounds")
        self.sound_manager = SoundManager(
            sound_directory=sound_dir,
            sound_enabled = self.user_config.sound_enabled,
            volume_multiplier = self.user_config.sound_volume
        )
        # Register shutdown to ensure PyAudio resources are released on program exit
        atexit.register(self.sound_manager.shutdown)
    def has_git_integration(self) -> bool:
        """Returns true if git integration is enabled for the current project.
            You still have to check for availability yourself, this just allows the project to disable it."""
        if self.project is None:
            return False

        return self.project.config.git_integration
            
    def _get_cli_prompt(self) -> str:
        """Returns the CLI prompt used in the interact command and any other REPL like interactions with the LLMs."""
        git_str = ""
        if self.has_git_integration():
            root = self.project_root if self.project_root else ""
            repo_gr = git.get_repo_name(root)
            branch_gr = git.get_current_branch(root)
            if repo_gr.value or branch_gr.value:
                git_str = f"[{repo_gr.value}:{branch_gr.value}] "
        
        # some ghostbox internal magic to get the token count
        coder_tokens = self.coder_box._plumbing._get_last_result_tokens()
        worker_tokens = self.worker_box._plumbing._get_last_result_tokens()
        return f" {coder_tokens} {worker_tokens} {git_str}>"

    def _has_api_keys(self) -> Dict[LLMBackend, bool]:
        """
        Compares the chosen backends to the user config and checks for required API keys.

        Returns:
            Dict[LLMBackend, bool]: A dictionary where keys are LLMBackend enum members
                                    and values are True if the API key is present, False otherwise.
                                    Only includes backends that are actually used and require keys.
        """
        if self.project is None:
            error_msg = (
                "Attempting to verify API keys with uninitialized project. Aborting."
            )
            logger.critical(error_msg)
            raise RuntimeError(error_msg)

        missing_keys: Dict[LLMBackend, bool] = {}

        # Check Coder LLM backend
        coder_backend_str = self.project.config.coder_backend
        if coder_backend_str == LLMBackend.google.name:
            if not self.user_config.google_api_key:
                missing_keys[LLMBackend.google] = False
            # else: # No need to add if key is present, we only care about missing ones
            #     missing_keys[LLMBackend.google] = True
        elif coder_backend_str == LLMBackend.openai.name:
            if not self.user_config.openai_api_key:
                missing_keys[LLMBackend.openai] = False
            # else:
            #     missing_keys[LLMBackend.openai] = True
        elif coder_backend_str == LLMBackend.generic.name:
            # For generic, if the endpoint is OpenAI or Google, we might need the general api_key
            # This is a heuristic, as generic can point to anything.
            # For now, we'll only check if the endpoint looks like OpenAI/Google official ones
            if "openai.com" in self.project.config.coder_endpoint:
                if not self.user_config.openai_api_key and not self.user_config.api_key:
                    missing_keys[LLMBackend.openai] = (
                        False  # Assume OpenAI key is preferred for OpenAI endpoints
                    )
                # else:
                #     missing_keys[LLMBackend.openai] = True
            elif "googleapis.com" in self.project.config.coder_endpoint:
                if not self.user_config.google_api_key and not self.user_config.api_key:
                    missing_keys[LLMBackend.google] = (
                        False  # Assume Google key is preferred for Google endpoints
                    )
                # else:
                #     missing_keys[LLMBackend.google] = True
            # If generic points to a local server (e.g., localhost:8080), no API key is expected.

        # Check Worker LLM backend
        worker_backend_str = self.project.config.worker_backend
        if worker_backend_str == LLMBackend.google.name:
            if not self.user_config.google_api_key:
                # Only add if not already marked as missing by coder_backend
                if LLMBackend.google not in missing_keys:
                    missing_keys[LLMBackend.google] = False
        elif worker_backend_str == LLMBackend.openai.name:
            if not self.user_config.openai_api_key:
                # Only add if not already marked as missing by coder_backend
                if LLMBackend.openai not in missing_keys:
                    missing_keys[LLMBackend.openai] = False
        elif worker_backend_str == LLMBackend.generic.name:
            if "openai.com" in self.project.config.worker_endpoint:
                if not self.user_config.openai_api_key and not self.user_config.api_key:
                    if LLMBackend.openai not in missing_keys:
                        missing_keys[LLMBackend.openai] = False
            elif "googleapis.com" in self.project.config.worker_endpoint:
                if not self.user_config.google_api_key and not self.user_config.api_key:
                    if LLMBackend.google not in missing_keys:
                        missing_keys[LLMBackend.google] = False

        # Filter out True entries, only return missing ones
        return {k: v for k, v in missing_keys.items() if not v}

    def discard_actions(self) -> None:
        """Empties the action queue."""
        if not (self.action_queue):
            logger.debug(f"Discard on empty action queue.")
            return

        actions_str = "\n".join(
            [json.dumps(action.model_dump(), indent=4) for action in self.action_queue]
        )
        logger.debug(
            f"Discard on action queue. Will discard the following actions:\n{actions_str}"
        )
        self.action_queue = []

    def queue_action(self, action: Action) -> None:
        """Queues an action at the end of the action queue."""
        logger.info(f"Queueing action {type(action)}.")
        self.action_queue.append(action)

    def push_front_action(self, action: Action) -> None:
        """Pushes an action to the front of the queue. An action at the front will be executed before the remaining ones."""
        logger.info(
            f"Pushing action {action_show_short(action)} to the front of the action queue."
        )
        self.action_queue = [action] + self.action_queue

    def confirm_action(
        self,
        action: Action,
        agent_clearance_level: ClearanceRequirement = ClearanceRequirement.AUTOMATIC,
        agent_name: str = "System",
    ) -> UserConfirmation:
        """Interactively acquire user confirmation for a given action.
        When an action needs to be confirmed, there is usually some agent who wants to perform the action (e.g. ghostcoder or ghostworker). The agent's current clearance level is provided for context.
        """
        self.sound_notify()
        self.print(f"{agent_name} wants to perform {action_show_short(action)}.")
        abridge = 80  # type: Optional[int]
        try:
            while True:
                choice = input(
                    "Permit? yes (y), no (n), yes to all (a), cancel all (q), or show more info (?, default):"
                )
                match choice:
                    case "y":
                        logger.info("User confirmed action.")
                        return UserConfirmation.YES
                    case "n":
                        logger.info("User denied action.")
                        return UserConfirmation.NO
                    case "a":
                        # FIXME: this should implement a default confirm for all confirmation dialogs that happen during the current run_action_queue execution.
                        logger.info("YOLO")
                        return UserConfirmation.ALL
                    case "q":
                        logger.info("User canceled all action requests.")
                        return UserConfirmation.CANCEL
                    case "d":
                        # secret debug option
                        print(
                            show_model_nt(
                                action, heading=action.__class__.__name__, abridge=None
                            )
                        )
                        continue
                    case _:
                        # cover "?", empty input, and anything else, as showing more nformation is generally a safe option.
                        self.print(
                            action_show_confirmation_data(
                                action,
                                first_showing=(abridge is not None),
                                abridge=abridge,
                            )
                        )
                        # we only show abridge once. user can do enter twice to see full strings
                        abridge = None
                        continue
        except EOFError as e:
            self.print(f"Canceled.")
            logger.info(
                f"User exited confirmation dialog with EOF. Defaulting to deny."
            )
            return UserConfirmation.CANCEL
        except Exception as e:
            self.print(f"Action canceled. Error: {e}.")
            logger.error(
                f"Encountered error during user confirmation dialog. Defaulting to deny. Error: {e}"
            )
            logger.debug(f"Full traceback:\n{traceback.format_exc()}")
            return UserConfirmation.CANCEL

        # unreachable but ok
        logger.info(f"Follow the white rabbit.")
        return UserConfirmation.CANCEL

    def print(
        self, text: str, end: str = "\n", tag: Optional[str] = None, flush: bool = True
    ) -> None:
        """Print a message to stdout.
        If you tag your print message, this print remembers and an automatic newline is inserted before another message that does not share the tag. This allows you to build up a line without interfereing in other print actions.
        """
        # we hijack the normal print so that we can more easily change things in the future
        # e.g. make it thread safe or keep a log.
        # right now it's just easy print

        if tag != self.last_print_tag and self.last_print_tag is not None:
            if self.last_print_tag == "action_queue":
                print("\r", end="", flush=True)
            else:
                print("")

        print(text, end=end, flush=flush)
        self.last_print_text = text
        self.last_print_tag = tag

    def make_tagged_printer(self, tag: str) -> Callable[[str], None]:
        return lambda text, end="\n", flush=True: self.print(text, end=end, flush=flush, tag=tag)  # type: ignore

    def announce(self, text: str, *, agent: AIAgent) -> None:
        """Alternative to print that announces some kind of system event to the user from the perspective of an AI agent.
        This method is intended to be used by deeply nested processing in the action queue to keep the user informed when unusual things happen. Announcements should therefore use simple language, and be used in addition to logging, not instead of it.
        """
        delimiter = colored(">>=", self.cosmetic_state.to_color())
        match agent:
            case AIAgent.WORKER:
                self.print(f"  {delimiter} {text}")
            case AIAgent.CODER:
                self.print(f"  {delimiter} {text}")
            case _ as unreachable:
                assert_never(unreachable)

    def get_data(self, relative_filepath: str) -> str:
        """Returns a filepath relative to the hidden ghostcode directory.
            Example: get_data("log.txt") -> ".ghostcode/log.txt"
        If no project root directory is defined, this function raises a runtime error.
        If the filepath that this function returns does not exist, no error is raised.
        """
        if self.project_root is None or not (
            os.path.isdir(
                ghostcode_dir := os.path.join(self.project_root, ".ghostcode")
            )
        ):
            msg = "Could not find .ghostcode directory. Please run `ghostcode init` and retry."
            logger.error(msg)
            raise RuntimeError(msg)

        return os.path.join(ghostcode_dir, relative_filepath)

    def get_config_or_default(self) -> ProjectConfig:
        """Returns the project config or a default config if retrieving the project config fails."""
        fail = ProjectConfig()

        if self.project is None:
            logger.warning(
                "Null project when trying to get config. This means that a default config is used."
            )
            return fail

        return self.project.config

    def show_log(self, tail: Optional[int] = None) -> str:
        """Returns the current event log if available.
        If tail is given, show only the tail latest number of log lines.
        """
        if self.project is None:
            logger.warn("Null project while trying to read logs.")
            return ""
        try:
            with open(self.get_data(self.project._LOG_FILE), "r") as f:
                log = f.read()

            if tail is None:
                return log

            return "\n".join([line for line in log.splitlines()][(-1) * tail :])

        except Exception as e:
            logger.warning(f"Failed to read log file. Reason: {e}")
            return ""


    @contextmanager        
    def sound_clicks(self, mean: float = 0.7) -> Generator[None, None, None]:
        """Plays clicking sounds if sound_enabled is true on the sound manager."""
        clicking_sounds = "clicks1.wav clicks2.wav clicks3.wav clicks_double.wav clicks_triple.wav".split(" ")
        with self.sound_manager.continuous_playback(
                clicking_sounds,
                mean = mean):
            yield

    def sound_error(self) -> None:
        self.sound_manager.play("error.wav")

    def sound_notify(self) -> None:
        self.sound_manager.play("opening1.wav")

    def debug_dump(self) -> None:
        """Save some debugging output into .ghostcode/debug/"""
        # FIXME: make this conditional on self.debug which should be set with --debug
        if self.project_root is None:
            logger.error(f"Cannot dump debug information: Project root is null.")
            return

        debug_dir = os.path.join(self.project_root, self._DEBUG_DIR)
        try:
            os.makedirs(debug_dir, exist_ok=True)
        except Exception as e:
            logger.exception(
                f"Couldn't create directories {debug_dir} while dumping debug information. Reason: {e}"
            )

        # we dump the logs for both boxes
        def save_box_history(box_name: str, box: Ghostbox) -> None:
            filename = f"{box_name}-{timestamp_now_iso8601()}"
            content = "\n---\n".join(
                [show_model_nt(chat_message) for chat_message in box.get_history()]
            )
            try:
                with open(os.path.join(debug_dir, filename), "w") as f:
                    f.write(content)
            except Exception as e:
                logger.exception(f"Couldn't dump {box_name} history. Reason: {e}")

        save_box_history("coder_box", self.coder_box)
        save_box_history("worker_box", self.worker_box)

    def get_current_backend_coder(self) -> LLMBackend:
        """Returns the current LLM backend for the coder.
        The current backend is determined in the order of options in: command line > user config > project config > defaults"""
        return self.coder_box.get("backend")
    
    def get_current_backend_worker(self) -> LLMBackend:
        """Returns the current LLM backend for the worker.
        The current backend is determined in the order of options in: command line > user config > project config > defaults"""
        return self.worker_box.get("backend")

    def get_current_model_coder(self) -> str:
        """Returns the currently used LLM model for the coder (if any is set). This is not the same as the backend.
        The model is determined in the order of: command line > user config > project config > default"""
        return self.coder_box.get("model")

    def get_current_model_worker(self) -> str:
        """Returns the currently used LLM model used by the worker (if any is set). This is not the same as the backend.
        The model is determined in the order of: command line > user config > project config > default"""
        return self.worker_box.get("model")    
    
